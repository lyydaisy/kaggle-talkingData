# Kaggle TalkingData AdTracking Fraud Detection Challenge 
48th Solution, competition link: https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection

1. [features.ipynb](https://github.com/shawnau/talkingData/blob/master/features.ipynb): notebook version
2. [train_xgb_lgb.py](https://github.com/shawnau/talkingData/blob/master/train_lgb_xgb.py): script version, gives about 0.9824 on private LB
3. blending.ipynb: blending historical models, which boost me about 0.0002
4. FTRL.ipynb: haven't tried due to limited time

**running this code on full training data needs 96GB RAM with 128G swap**

## Some Solutions as a Reference

1. [3th, NN model](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56262#latest-325349)
2. [4th](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56243#latest-325397)
3. [6th, strong single lightGBM](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion)
4. [9th](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56279#latest-325405)
5. [FFM trick](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56282)

## Train log
please see the [dashboard](https://github.com/shawnau/talkingData/projects/1)
